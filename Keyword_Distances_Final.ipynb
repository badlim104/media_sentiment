{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to calculate the cosine distances between keywords of NYT and Reuters articles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.spatial.distance\n",
    "\n",
    "#this is a function that both tokenizes a given sentence/article \n",
    "def token(text): \n",
    "    # tokenize by sentence and word. this way you ensure you get rid of punctuations\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens_with_letters = []\n",
    "    # use the regex library to search only for items that contain letters. this will enable you to eliminate punctuation\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            tokens_with_letters.append(token)\n",
    "    return tokens_with_letters\n",
    "\n",
    "#makes a single list out of list of lists with string elements in it\n",
    "def flatten(foo):\n",
    "    for x in foo:\n",
    "        if hasattr(x, '__iter__') and not isinstance(x, str):\n",
    "            for y in flatten(x):\n",
    "                yield y\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "#function to get rid of paranthesis and trailing white spaces in the keywords             \n",
    "def rid_paranthesis_trailing_wspace(words):\n",
    "    for i in range(len(words)): \n",
    "        if \"(\" in words[i]:\n",
    "            words[i]=re.match(\"(.*?)\\(\",words[i]).group(1) \n",
    "            words[i]=words[i].rstrip()            \n",
    "        else: \n",
    "            words[i]=words[i].rstrip()\n",
    "    return words \n",
    "\n",
    "#function to get rid of the \\ character in the reuters keywords. treat words separated by this character as two words \n",
    "def rid_special_char(words):\n",
    "    for i in range(len(words)):\n",
    "        if \"/\" in words[i]:\n",
    "            words[i]=words[i].split('/')\n",
    "    return list(flatten(words))\n",
    "\n",
    "#this one makes a sentence out of a list of keywords\n",
    "def make_sentence_out_of_list(words_list):\n",
    "    sentence=' '.join(word for word in words_list)\n",
    "    return sentence\n",
    "\n",
    "#read the reuters and nyt dataframes\n",
    "nyt_unique=pd.read_csv('nyt_uniques_topicsLabeled.csv')\n",
    "nyt_labeled_only=nyt_unique.loc[nyt_unique['Section'] != 'Other']\n",
    "nyt_labeled_only.reset_index(inplace=True)\n",
    "nyt_labeled_only.drop('index',axis=1,inplace=True)\n",
    "reuters_unique = pickle.load( open( \"20180516-20180621_reuters_unique.pkl\", \"rb\" ) )\n",
    "reuters_unique.reset_index(inplace=True)\n",
    "reuters_unique.drop('index',axis=1,inplace=True)\n",
    "\n",
    "#there is one row that doesn't have keywords for NYT. \n",
    "#Might as well manually fix that so we don't run into indexing issues: \n",
    "nyt_labeled_only.Keywords[92]='nan'\n",
    "\n",
    "#for reuters, when we read from the pkl, the keywords are already in \"list\" format. in NYT this is not the case \n",
    "#for one reason or the other. this line of code puts the keywords in NYT in a list, split by a comma\n",
    "nyt_labeled_only['keyword list']=nyt_labeled_only['Keywords'].apply(lambda x: x.split(',') if(np.all(pd.notnull(x))) else x)\n",
    "\n",
    "#nyt has \"null\" values, so we manually make sentences out of lists of keywords by putting an \"if\" statement \n",
    "nyt_labeled_only['sentences']=nyt_labeled_only['keyword list'].apply(lambda words_list:' '.join(word for word in words_list) if(np.all(pd.notnull(words_list))) else words_list)\n",
    "\n",
    "#only reuters needs the functions below. we first get rid of \"\\\"\n",
    "#we then get rid of paranthesis and whitespace (because whitespace remains after applying the \"rid_special_char\" function)\n",
    "reuters_unique.loc[:, 'keywords'] = reuters_unique.keywords.apply(rid_special_char)\n",
    "reuters_unique.loc[:, 'keywords'] = reuters_unique.keywords.apply(rid_paranthesis_trailing_wspace)\n",
    "\n",
    "#we then apply the 'make sentences out of list'. we can apply it here directly bc there are no null values. \n",
    "reuters_unique['sentences']=reuters_unique['keywords'].apply(make_sentence_out_of_list)\n",
    "\n",
    "#make north korea one word: 'nkorea'\n",
    "#make donald trump one word: 'dtrump'\n",
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('north korea', 'nkorea', regex=False)\n",
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('northkorea', 'nkorea', regex=False)\n",
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('nothkorea', 'nkorea', regex=False)\n",
    "nyt_labeled_only.loc[:, 'sentences'] = nyt_labeled_only.sentences.str.replace('north korea', 'nkorea', regex=False)\n",
    "nyt_labeled_only.loc[:, 'sentences'] = nyt_labeled_only.sentences.str.replace('donald trump', 'dtrump', regex=False)\n",
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('donald trump', 'dtrump', regex=False)\n",
    "\n",
    "#put the nyt and reuters keywords into one big list\n",
    "nyt_list=nyt_labeled_only['sentences'].tolist()\n",
    "reuters_list=reuters_unique['sentences'].tolist()\n",
    "nyt_list.extend(reuters_list)\n",
    "\n",
    "#put the keywords into a term frequency matrix\n",
    "vectorizer = TfidfVectorizer(use_idf=False, norm='l2',tokenizer=token, ngram_range=(1,3))\n",
    "tfidf_matrix = vectorizer.fit_transform(nyt_list)\n",
    "\n",
    "#separate the NYT and Reuters matrices. You will calculate distances between the\n",
    "#elements of these matrices\n",
    "nyt_keywords_tfidf=tfidf_matrix[0:1400,:].toarray()\n",
    "reuters_keywords_tfidf=tfidf_matrix[1400:,:].toarray()\n",
    "\n",
    "#calculate the distances between keywords. dump into pickle\n",
    "keyword_distances=scipy.spatial.distance.cdist(nyt_keywords_tfidf,reuters_keywords_tfidf,'cosine')\n",
    "qf=open('keyword_distances.pkl','wb')\n",
    "pickle.dump(keyword_distances,qf)\n",
    "\n",
    "#add keyword distances to your big df\n",
    "big_df = pickle.load(open( \"big_df.pkl\", \"rb\" ))\n",
    "flatten = lambda l:[item for sublist in l for item in sublist]\n",
    "big_df['keyword_distances']=pd.Series(flatten(keyword_distances.tolist()))\n",
    "wf=open('big_df.pkl','wb')\n",
    "pickle.dump(big_df,wf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
