{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to get the links, titles, articles and keywords for NYT's front page\n",
    "#example link for the front page: https://www.nytimes.com/issue/todayspaper/2018/12/15/todays-new-york-times\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#DATE is in YYYYMMDD format\n",
    "#get two dates from user:start date and end date. Get all links for the front page between these dates.\n",
    "def get_links_nyt():\n",
    "    #get input\n",
    "    start_date = datetime.strptime(input(\"Please enter the start date:\"), '%Y%m%d')\n",
    "    end_date=datetime.strptime(input(\"Please enter the end date:\"), '%Y%m%d')\n",
    "    #create list of dates between start and end dates\n",
    "    delta=end_date-start_date\n",
    "    date_list=[start_date + timedelta(i) for i in range(delta.days + 1)]\n",
    "    link_list=[]\n",
    "    #scrape what the links of articles for each date. put them in a list.\n",
    "    for i in date_list:\n",
    "        try:\n",
    "            p=urllib.request.urlopen(' https://www.nytimes.com/issue/todayspaper/'+i.strftime('%Y/%m/%d/')+'todays-new-york-times').read()  \n",
    "        except Exception as e:\n",
    "            link_list.append(e)\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(p,'lxml')\n",
    "        content=soup.find_all(\"h2\",class_=\"headline\") \n",
    "        print(len(content))\n",
    "        for i in content:\n",
    "            link_list.append(i.a['href'])            \n",
    "            \n",
    "                \n",
    "    return link_list\n",
    "\n",
    "#get the date of each article, put them in a list \n",
    "def get_dates_nyt(links):\n",
    "    dates_list=[]\n",
    "    for i in links:\n",
    "        try:\n",
    "            y=urllib.request.urlopen(i).read()\n",
    "        except Exception as e:\n",
    "            dates_list.append(e)\n",
    "            continue\n",
    "        z = BeautifulSoup(y,'lxml')\n",
    "        a_dates=z.find_all('meta',attrs={'itemprop':'datePublished'})\n",
    "        print(a_dates)\n",
    "        if len(a_dates)>=1:\n",
    "            dates_list.append(a_dates[0]['content'].lower().split('T'))\n",
    "        elif len(a_dates)==0:\n",
    "            dates_list.append(\"Na\")                        \n",
    "    return dates_list\n",
    "\n",
    "#get the keywords of each article, put them in a list \n",
    "def get_keywords_nyt(links):\n",
    "    keywords_list=[]\n",
    "    for i in links:\n",
    "        try:\n",
    "            y=urllib.request.urlopen(i).read()\n",
    "        except Exception as e:\n",
    "            keywords_list.append(e)\n",
    "            continue\n",
    "        z = BeautifulSoup(y,'lxml')\n",
    "        a_keywords=z.find_all('meta',attrs={'name':'news_keywords'})\n",
    "        if len(a_keywords)>=1:\n",
    "            keywords_list.append(a_keywords[0]['content'].lower())\n",
    "        elif len(a_keywords)==0:\n",
    "            keywords_list.append(\"Na\")                        \n",
    "    return keywords_list\n",
    "\n",
    "#get the bodies of each article. put them on a list \n",
    "def get_articles_nyt(links):\n",
    "    article_list=[]\n",
    "    for i in links:\n",
    "        try:\n",
    "            y=urllib.request.urlopen(i).read()\n",
    "       \n",
    "        except Exception as e:\n",
    "            article_list.append(e)\n",
    "            continue\n",
    "        \n",
    "        z = BeautifulSoup(y,'lxml')\n",
    "        a=z.find_all(\"div\",class_=\"css-18sbwfn StoryBodyCompanionColumn\")\n",
    "        if len(a)>=1:\n",
    "            article_list.append(' '.join(paragraph.get_text() for paragraph in a))\n",
    "        elif len(a)==0:\n",
    "            article_list.append('Na')\n",
    "    return article_list\n",
    "\n",
    "#get the titles for each article. put them in a list \n",
    "def get_titles_nyt(links):\n",
    "    title_list=[]\n",
    "    for i in links:\n",
    "        try:\n",
    "            y=urllib.request.urlopen(i).read()     \n",
    "        except Exception as e:\n",
    "            title_list.append(e)\n",
    "            continue\n",
    "        z = BeautifulSoup(y,'lxml')\n",
    "        a_titles=z.find_all(\"title\")\n",
    "        if len(a_titles)>=1:\n",
    "            title_list.append(a_titles[0].get_text()[0:-21])\n",
    "        elif len(a)==0:\n",
    "            title_list.append(\"Na\")            \n",
    "    return title_list\n",
    "\n",
    "\n",
    "\n",
    "#run all functions, get all elements \n",
    "links=get_links_nyt()\n",
    "articles=get_articles_nyt(links)\n",
    "keywords=get_keywords_nyt(links)\n",
    "titles=get_titles_nyt(links)\n",
    "dates=get_dates_nyt(links)\n",
    "\n",
    "#put all elements in a pandas dataframe as separate columns\n",
    "df_nyt = pd.DataFrame(np.column_stack([dates, links, keywords,articles,titles]),columns=['date', 'link', 'keywords','article','title']) \n",
    "\n",
    "#write dataframe in a csv \n",
    "df_nyt.to_csv(\"20180516-20180621_nyt.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
