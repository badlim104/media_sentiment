{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "CPU times: user 5.18 s, sys: 416 ms, total: 5.59 s\n",
      "Wall time: 2.61 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347046"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import scipy.spatial.distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "#read the files containing Reuters and NYT articles \n",
    "nyt_unique=pd.read_csv('nyt_uniques_topicsLabeled.csv')\n",
    "reuters_unique = pickle.load( open( \"20180516-20180621_reuters_unique.pkl\", \"rb\" ) )\n",
    "\n",
    "#this is a function to both tokenize and stem a given body of text\n",
    "def token_stem(text):\n",
    "    # tokenize by sentence and word. this way you ensure you get rid of punctuations\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens_with_letters = []\n",
    "    # use the regex library to search only for items that contain letters. \n",
    "    #this will enable you to eliminate punctuation\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            tokens_with_letters.append(token)\n",
    "    stems = [stemmer.stem(t) for t in tokens_with_letters] #\"stems\" part\n",
    "    return stems\n",
    "\n",
    "#reset indices for both dataframes \n",
    "#also select only article that are not labeled as \"Other\" from the NYT dataframe\n",
    "reuters_unique.reset_index(inplace=True)\n",
    "reuters_unique.drop('index',axis=1,inplace=True)\n",
    "nyt_labeled_only=nyt_unique.loc[nyt_unique['Section'] != 'Other'].copy()\n",
    "nyt_labeled_only.reset_index(inplace=True)\n",
    "nyt_labeled_only.drop('index',axis=1,inplace=True)\n",
    "\n",
    "#put the Reuters and NYT articles in lists\n",
    "nyt_articles=nyt_labeled_only['Article'].tolist()\n",
    "reuters_articles=reuters_unique['article'].tolist()\n",
    "\n",
    "#make one big list of NYT articles and Reuters articles \n",
    "nyt_articles.extend(reuters_articles)\n",
    "\n",
    "#These articles have weird characters in them. This is probably an encoding issue \n",
    "#Resort to cleaning them manually. \n",
    "nyt_articles=list(map(lambda unicode_line:unicode_line.translate({ord(c): None for c in 'Äòôúîù'}),nyt_articles))\n",
    "\n",
    "#Use the nltk library's stopwords corpus and SnowballStemmer's stemmer \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "#vectorize the articles. this is where you specify important parameters such as \n",
    "#what type of tokenizer to use, whether you use idf or not etc.\n",
    "vectorizer = TfidfVectorizer(stop_words='english',use_idf=True, norm='l2',tokenizer=token_stem, ngram_range=(1,3))\n",
    "\n",
    "#put the vectors in a matrix \n",
    "tfidf_matrix = vectorizer.fit_transform(nyt_articles)\n",
    "\n",
    "#separate the matrix into one for NYT and another for Reuter articles. \n",
    "tfidf_1=tfidf_matrix[0:1400,:]\n",
    "tfidf_2=tfidf_matrix[1400:,:]\n",
    "\n",
    "#calculate cosine similarities between Reuters and NYT articles \n",
    "similarities_dense = cosine_similarity(tfidf_1,tfidf_2,dense_output=True)\n",
    "\n",
    "#we need the distances. so subtract similarities from 1\n",
    "article_distances=1-article_similarities\n",
    "\n",
    "#now, create a new column in your big_df and call it article_distances\n",
    "big_df = pickle.load( open( \"big_df.pkl\", \"rb\" ) )\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "big_df['article_distances']=pd.Series(flatten(article_distances.tolist()))\n",
    "\n",
    "#dump big_df back to where it belongs\n",
    "wf=open('big_df.pkl','wb')\n",
    "pickle.dump(big_df,wf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
