{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script for the Reuters scraper. Uses the Reuters News Archive (now defunct)\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "#function to get the links from https://www.reuters.com/resources/archive/us/DATE.html\n",
    "#DATE is in YYYYMMDD format\n",
    "#get two dates from user:start date and end date. Get all links for articles between these dates\n",
    "def get_links():\n",
    "    #get input\n",
    "    start_date = datetime.strptime(input(\"Please enter the start date:\"), '%Y%m%d')\n",
    "    end_date=datetime.strptime(input(\"Please enter the end date:\"), '%Y%m%d')\n",
    "    #create list of dates between start and end dates\n",
    "    delta=end_date-start_date\n",
    "    date_list=[start_date + timedelta(i) for i in range(delta.days + 1)]\n",
    "    link_list=[]\n",
    "    #scrape what the links of articles for each date. put them in a list.\n",
    "    for i in date_list:\n",
    "        p = urllib.request.urlopen('https://www.reuters.com/resources/archive/us/'+i.strftime('%Y%m%d')+'.html').read()\n",
    "        soup = BeautifulSoup(p,'lxml')\n",
    "        content=soup.find_all(\"div\",class_=\"module\")\n",
    "        if len(content)>=1:\n",
    "            for i in content[0].find_all(class_='headlineMed'): \n",
    "                link_list.append(i.a['href'])            \n",
    "    return link_list\n",
    "\n",
    "#function to get the dates of each article\n",
    "#use the link list created by the \"get_links\" function and get timestamps of each article\n",
    "def get_dates(links):\n",
    "    date_list=[]\n",
    "    for i in links:\n",
    "        try:\n",
    "            y=urllib.request.urlopen(i).read()  \n",
    "        except Exception as e:\n",
    "            date_list.append(e)\n",
    "            continue\n",
    "        z = BeautifulSoup(y,'lxml')\n",
    "        a=z.find_all(\"div\",class_=\"date_V9eGk\")\n",
    "        #print(len(a))\n",
    "        if len(a)>=1:\n",
    "            date_list.append(a[0].get_text()[0:23])\n",
    "        elif len(a)==0:\n",
    "            date_list.append(\"Na\")                    \n",
    "    return date_list\n",
    "\n",
    "#function to get the keywords of each article\n",
    "#use the link list created by the \"get_links\" function and get keywords of each article\n",
    "def get_keywords(links):\n",
    "    keyword_list=[]\n",
    "    for i in links:\n",
    "        try:\n",
    "            y=urllib.request.urlopen(i).read()\n",
    "        except Exception as e:\n",
    "            keyword_list.append(e)\n",
    "            continue\n",
    "        z = BeautifulSoup(y,'lxml')\n",
    "        a_keys=z.find_all('meta',attrs={'name':'keywords'})\n",
    "        #print(len(a))\n",
    "        if len(a_keys)>=1:\n",
    "            keyword_list.append(a_keys[0]['content'].lower().split(','))\n",
    "        elif len(a)==0:\n",
    "            keyword_list.append(\"Na\")                        \n",
    "    return keyword_list\n",
    "\n",
    "\n",
    "#function to get the articles \n",
    "#use the link list created by the \"get_links\" function and get titles of each article\n",
    "def get_articles(links):\n",
    "    article_list=[]\n",
    "    for i in links:\n",
    "        try:\n",
    "            y=urllib.request.urlopen(i).read()\n",
    "       \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        z = BeautifulSoup(y,'lxml')\n",
    "        a=z.find_all(\"div\",class_=\"body_1gnLA\")\n",
    "        #print(len(a))\n",
    "        if len(a)>=1:\n",
    "            start_time = time.time()\n",
    "            article_list.append(a[0].get_text())\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        elif len(a)==0:\n",
    "            article_list.append('Na')\n",
    "    return article_list\n",
    "        \n",
    "#run the functions\n",
    "links=get_links()\n",
    "keywords=get_keywords(links)\n",
    "dates=get_dates(links)\n",
    "articles=get_articles(links)\n",
    "\n",
    "#put everything in a n * 3 dataframe where n is the number of articles and the columns are dates, links and keywords\n",
    "#respectively. you need to do some cleaning to get to the appropriate format of the dates. \n",
    "df_1 = pd.DataFrame(np.column_stack([dates, links, keywords,articles]),columns=['date', 'link', 'keywords','articles']) \n",
    "df_1['date'] = df_1['date'].astype(str)\n",
    "df_1['new']=df_1.loc[:,'date'].str[0:23]\n",
    "df_1.drop('date',axis=1,inplace=True)\n",
    "df_1.rename(columns={'new':'date'}, inplace=True)\n",
    "\n",
    "#put it in a csv\n",
    "df_1.to_csv('reuters_all_articles.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
