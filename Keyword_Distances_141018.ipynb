{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berkayadlim/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "/Users/berkayadlim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/berkayadlim/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/berkayadlim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/berkayadlim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.externals import joblib\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#this is a function that both tokenizes and stems a given sentence/article \n",
    "def token_stem(text):\n",
    "    # tokenize by sentence and word. this way you ensure you get rid of punctuations\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens_with_letters = []\n",
    "    # use the regex library to search only for items that contain letters. this will enable you to eliminate punctuation\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            tokens_with_letters.append(token)\n",
    "    stems = [stemmer.stem(t) for t in tokens_with_letters] #\"stems\" part\n",
    "    return stems\n",
    "\n",
    "#difference between this one and the one above is the \"stems\" part. this function is a tokenizer\n",
    "def token(text): \n",
    "    #same as above\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens_with_letters = []\n",
    "    # same as above\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            tokens_with_letters.append(token)\n",
    "    return tokens_with_letters\n",
    "\n",
    "def flatten(foo):\n",
    "    for x in foo:\n",
    "        if hasattr(x, '__iter__') and not isinstance(x, str):\n",
    "            for y in flatten(x):\n",
    "                yield y\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "#function to get rid of paranthesis and trailing white spaces in the keywords             \n",
    "def rid_paranthesis_trailing_wspace(words):\n",
    "    for i in range(len(words)): \n",
    "        if \"(\" in words[i]:\n",
    "            words[i]=re.match(\"(.*?)\\(\",words[i]).group(1) \n",
    "            words[i]=words[i].rstrip()            \n",
    "        else: \n",
    "            words[i]=words[i].rstrip()\n",
    "    return words \n",
    "\n",
    "#function to get rid of the \\ character in the reuters keywords. treat words separated by this character as two words \n",
    "def rid_special_char(words):\n",
    "    for i in range(len(words)):\n",
    "        if \"/\" in words[i]:\n",
    "            words[i]=words[i].split('/')\n",
    "    return list(flatten(words))\n",
    "\n",
    "def make_sentence_out_of_list(words_list):\n",
    "    sentence=' '.join(word for word in words_list)\n",
    "    return sentence\n",
    "\n",
    "nyt_unique=pd.read_csv('nyt_uniques_topicsLabeled.csv')\n",
    "nyt_labeled_only=nyt_unique.loc[nyt_unique['Section'] != 'Other']\n",
    "nyt_labeled_only.reset_index(inplace=True)\n",
    "nyt_labeled_only.drop('index',axis=1,inplace=True)\n",
    "reuters_unique = pickle.load( open( \"20180516-20180621_reuters_unique.pkl\", \"rb\" ) )\n",
    "reuters_unique.reset_index(inplace=True)\n",
    "reuters_unique.drop('index',axis=1,inplace=True)\n",
    "\n",
    "#there is one row that doesn't have keywords for NYT. Might as well manually fix that so we don't run into indexing issues: \n",
    "nyt_labeled_only.Keywords[92]='nan'\n",
    "#for reuters, when we read from the pkl, the keywords are already in \"list\" format. in NYT this is not the case \n",
    "#for one reason or the other. this line of code puts the keywords in NYT in a list, split by a comma\n",
    "nyt_labeled_only['keyword list']=nyt_labeled_only['Keywords'].apply(lambda x: x.split(',') if(np.all(pd.notnull(x))) else x)\n",
    "#nyt has \"null\" values, so we manually make sentences out of lists of keywords by putting an \"if\" statement \n",
    "nyt_labeled_only['sentences']=nyt_labeled_only['keyword list'].apply(lambda words_list:' '.join(word for word in words_list) if(np.all(pd.notnull(words_list))) else words_list)\n",
    "#only reuters needs the functions below. we first get rid of \"\\\"\n",
    "#we then get rid of paranthesis and whitespace (because whitespace remains after applying the \"rid_special_char\" function)\n",
    "reuters_unique.loc[:, 'keywords'] = reuters_unique.keywords.apply(rid_special_char)\n",
    "reuters_unique.loc[:, 'keywords'] = reuters_unique.keywords.apply(rid_paranthesis_trailing_wspace)\n",
    "#we then apply the 'make sentences out of list'. we can apply it here directly bc there are no null values. \n",
    "reuters_unique['sentences']=reuters_unique['keywords'].apply(make_sentence_out_of_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('north korea', 'nkorea', regex=False)\n",
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('northkorea', 'nkorea', regex=False)\n",
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('nothkorea', 'nkorea', regex=False)\n",
    "nyt_labeled_only.loc[:, 'sentences'] = nyt_labeled_only.sentences.str.replace('north korea', 'nkorea', regex=False)\n",
    "nyt_labeled_only.loc[:, 'sentences'] = nyt_labeled_only.sentences.str.replace('donald trump', 'dtrump', regex=False)\n",
    "reuters_unique.loc[:, 'sentences'] = reuters_unique.sentences.str.replace('donald trump', 'dtrump', regex=False)\n",
    "\n",
    "nyt_list=nyt_labeled_only['sentences'].tolist()\n",
    "reuters_list=reuters_unique['sentences'].tolist()\n",
    "nyt_list.extend(reuters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 ms, sys: 22.9 ms, total: 49.2 ms\n",
      "Wall time: 76.2 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=False, norm='l2',tokenizer=token, ngram_range=(1,3))\n",
    "%time tfidf_matrix = vectorizer.fit_transform(nyt_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
