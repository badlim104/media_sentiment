{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "CPU times: user 5.18 s, sys: 416 ms, total: 5.59 s\n",
      "Wall time: 2.61 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347046"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import scipy.spatial.distance\n",
    "import numba \n",
    "from numba import jit\n",
    "\n",
    "\n",
    "\n",
    "nyt_unique=pd.read_csv('nyt_uniques_topicsLabeled.csv')\n",
    "reuters_unique = pickle.load( open( \"20180516-20180621_reuters_unique.pkl\", \"rb\" ) )\n",
    "\n",
    "def token_stem(text):\n",
    "    # tokenize by sentence and word. this way you ensure you get rid of punctuations\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens_with_letters = []\n",
    "    # use the regex library to search only for items that contain letters. this will enable you to eliminate punctuation\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            tokens_with_letters.append(token)\n",
    "    stems = [stemmer.stem(t) for t in tokens_with_letters] #\"stems\" part\n",
    "    return stems\n",
    "\n",
    "reuters_unique.reset_index(inplace=True)\n",
    "reuters_unique.drop('index',axis=1,inplace=True)\n",
    "nyt_labeled_only=nyt_unique.loc[nyt_unique['Section'] != 'Other'].copy()\n",
    "nyt_labeled_only.reset_index(inplace=True)\n",
    "nyt_labeled_only.drop('index',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "nyt_articles=nyt_labeled_only['Article'].tolist()\n",
    "reuters_articles=reuters_unique['article'].tolist()\n",
    "nyt_articles.extend(reuters_articles)\n",
    "\n",
    "nyt_articles=list(map(lambda unicode_line:unicode_line.translate({ord(c): None for c in 'Äòôúîù'}),nyt_articles))\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english',use_idf=True, norm='l2',tokenizer=token_stem, ngram_range=(1,3))\n",
    "tfidf_matrix = vectorizer.fit_transform(nyt_articles)\n",
    "tfidf_1=tfidf_matrix[0:1400,:]\n",
    "tfidf_2=tfidf_matrix[1400:,:]\n",
    "print(type(tfidf_matrix))\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "#%time similarities_sparse = cosine_similarity(tfidf_1,tfidf_2,dense_output=False)\n",
    "%time similarities_dense = cosine_similarity(tfidf_1,tfidf_2,dense_output=True)\n",
    "np.count_nonzero(similarities_dense==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 22322)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "article_similarities = pickle.load( open( \"article_similarities.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_distances=1-article_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31250800, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df = pickle.load( open( \"big_df.pkl\", \"rb\" ) )\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "big_df['article_distances']=pd.Series(flatten(article_distances.tolist()))\n",
    "big_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf=open('big_df.pkl','wb')\n",
    "pickle.dump(big_df,wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
